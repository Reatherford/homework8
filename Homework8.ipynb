{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "##\"1.10. Decision Trees\" section from the scikit-learn documentation to recreate the examples in a Jupyter Notebook with detailed comments for each line.\n",
    "\n",
    "## Jupyter Notebook Implementation with Enhanced Explanations\n",
    "In this notebook:\n",
    "\n",
    "1. Basic classification example using a small sample dataset, to demonstrate training and prediction.\n",
    "2. A real-world example using the Iris dataset, showcasing how to load the dataset, train a classifier, and visualize the resulting decision tree.\n",
    "3. For visualization, the code includes comments for both `graphviz` and `matplotlib` methods. If you have `graphviz` (Homebrew(brew install graphviz)) to congig your system, you can uncomment the `graphviz` section to view the tree with more details. Otherwise, the `matplotlib` section will generate a visual representation of the tree. \n",
    "4. Each line of code is annotated with comments to explain its purpose, making it easier to understand how decision trees work within the scikit-learn library. \n",
    "5. This covers the key aspects discussed up to section \"1.10.1. Classification\" in the provided text.\n",
    "\n",
    "Make sure you have the required packages:\n",
    "\n",
    "```bash\n",
    "pip install scikit-learn graphviz matplotlib python-graphviz ipykernel\n",
    "pip install notebook \n",
    "```\n",
    "\n",
    "```python\n",
    "# 1.10. Decision Trees\n",
    "# Decision Trees (DTs) are used for classification and regression tasks, making decisions based on data features.\n",
    "\n",
    "# ---------------------------------\n",
    "# 1.10.1. Classification\n",
    "# ---------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install scikit-learn graphviz matplotlib ipykernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree  # Import the tree module for decision tree functionalities.\n",
    "from sklearn.datasets import load_iris  # Import load_iris to get the iris dataset.\n",
    "\n",
    "# Example 1: Basic Classification with DecisionTreeClassifier\n",
    "X = [[0, 0], [1, 1]]  # Sample feature data.\n",
    "Y = [0, 1]  # Corresponding target labels.\n",
    "\n",
    "clf = tree.DecisionTreeClassifier()  # Initialize a decision tree classifier.\n",
    "clf = clf.fit(X, Y)  # Train the classifier with the sample data.\n",
    "\n",
    "prediction = clf.predict([[2., 2.]])  # Predict the class for a new data point.\n",
    "print(prediction)  # Print the predicted class, which is [1].\n",
    "\n",
    "probabilities = clf.predict_proba([[2., 2.]])  # Predict probabilities for each class.\n",
    "print(probabilities)  # Print the predicted probabilities, which is [[0., 1.]].\n",
    "\n",
    "# Example 2: Using the Iris Dataset\n",
    "iris = load_iris()  # Load the iris dataset.\n",
    "X, y = iris.data, iris.target  # Assign features (X) and target (y).\n",
    "\n",
    "clf = tree.DecisionTreeClassifier()  # Initialize another decision tree classifier.\n",
    "clf = clf.fit(X, y)  # Train the classifier with the iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.plot_tree(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook provides examples of:\n",
    " ## Basic Decision Tree Training and Visualization: Trains a basic DecisionTreeClassifier and visualizes the trained tree structure using matplotlib.\n",
    "# Train/Test Split and Evaluation: Demonstrates how to split your data into training and testing sets and evaluate the model's performance using the score method (which calculates accuracy for classification).\n",
    "Cross-Validation: Shows how to use cross_val_score for more robust evaluation using k-fold cross-validation.\n",
    "Parameter Tuning (max_depth example): Provides a simple example of tuning the max_depth parameter using cross-validation to find the optimal depth that balances model complexity and performance. You can apply this same approach to tune other hyperparameters like min_samples_split, min_samples_leaf, etc.\n",
    "Feature Importance: Shows how to access the feature_importances_ attribute to understand which features are most influential in the decision-making process.\n",
    "Cost Complexity Pruning: Demonstrates finding the optimal pruning parameter (ccp_alpha) using cost complexity pruning and cross-validation, which helps avoid overfitting. The example uses the training dataset to find the most appropriate value for alpha and then visualizes the pruned tree and outputs its feature importances. Additionally, it plots the accuracy scores against alpha values for training and test sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides examples of:\n",
    "\n",
    "# Basic Decision Tree Training and Visualization: \n",
    "Trains a basic DecisionTreeClassifier and visualizes the trained tree structure using matplotlib.\n",
    "\n",
    "# Train/Test Split and Evaluation: \n",
    "Demonstrates how to split your data into training and testing sets and evaluate the model's performance using the score method (which calculates accuracy for classification).\n",
    "\n",
    "# Cross-Validation: \n",
    "Shows how to use cross_val_score for more robust evaluation using k-fold cross-validation.\n",
    "\n",
    "# Parameter Tuning (max_depth example): \n",
    "Provides a simple example of tuning the max_depth parameter using cross-validation to find the optimal depth that balances model complexity and performance. You can apply this same approach to tune other hyperparameters like min_samples_split, min_samples_leaf, etc.\n",
    "\n",
    "# Feature Importance: \n",
    "Shows how to access the feature_importances_ attribute to understand which features are most influential in the decision-making process.\n",
    "\n",
    "# Cost Complexity Pruning: \n",
    "Demonstrates finding the optimal pruning parameter (ccp_alpha) using cost complexity pruning and cross-validation, which helps avoid overfitting. The example uses the training dataset to find the most appropriate value for alpha and then visualizes the pruned tree and outputs its feature importances. Additionally, it plots the accuracy scores against alpha values for training and test sets.\n",
    "\n",
    "This comprehensive example notebook will help you get a better understanding of DecisionTreeClassifier and its various functionalities, including training, evaluation, visualization, parameter tuning, and pruning techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
